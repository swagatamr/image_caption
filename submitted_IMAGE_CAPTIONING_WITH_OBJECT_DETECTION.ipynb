{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LOADING AND IMPORTING ALL LIBRARIES FOR IMAGE CAPTIONING"
      ],
      "metadata": {
        "id": "By7_mGNiswoU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCnIaQFL0R4o"
      },
      "outputs": [],
      "source": [
        "!pip install transformers rouge_score evaluate datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece\n"
      ],
      "metadata": {
        "id": "RnJ6yhOEjTuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eK_JAStp4ada"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import *\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FUNCTIONS FOR IMAGE CAPTIONING"
      ],
      "metadata": {
        "id": "fCiW2ebNkOu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load a fine-tuned image captioning model and corresponding tokenizer and image processor\n",
        "finetuned_model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\").to(device)\n",
        "finetuned_tokenizer = GPT2TokenizerFast.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "finetuned_image_processor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")"
      ],
      "metadata": {
        "id": "uXXi51Pby2FP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxWYldgKTVw8"
      },
      "outputs": [],
      "source": [
        "import urllib.parse as parse\n",
        "import os\n",
        "\n",
        "# a function to determine whether a string is a URL or not\n",
        "def is_url(string):\n",
        "    try:\n",
        "        result = parse.urlparse(string)\n",
        "        return all([result.scheme, result.netloc, result.path])\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "# a function to load an image\n",
        "def load_image(image_path):\n",
        "    if is_url(image_path):\n",
        "        return Image.open(requests.get(image_path, stream=True).raw)\n",
        "    elif os.path.exists(image_path):\n",
        "        return Image.open(image_path)\n",
        "\n",
        "\n",
        "# a function to perform inference\n",
        "def get_caption(model, image_processor, tokenizer, image_path):\n",
        "    image = load_image(image_path)\n",
        "    # preprocess the image\n",
        "    img = image_processor(image, return_tensors=\"pt\").to(device)\n",
        "    # generate the caption (using greedy decoding by default)\n",
        "    output = model.generate(**img)\n",
        "    # decode the output\n",
        "    caption = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
        "    return caption"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using the pipeline API\n",
        "image_captioner = pipeline(\"image-to-text\", model=\"Abdou/vit-swin-base-224-gpt2-image-captioning\")\n",
        "image_captioner.model = image_captioner.model.to(device)"
      ],
      "metadata": {
        "id": "Q9bom73L7d5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FOR DETECTION"
      ],
      "metadata": {
        "id": "cr8ahJLSzUwx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## yolov8"
      ],
      "metadata": {
        "id": "RGG3-67mce1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ],
      "metadata": {
        "id": "Qef_886TcggT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "%cd {HOME}\n",
        "!git clone https://github.com/ultralytics/ultralytics\n",
        "%cd {HOME}/ultralytics\n",
        "!pip install -e .\n",
        "\n",
        "from IPython import display\n",
        "display.clear_output()\n",
        "\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ],
      "metadata": {
        "id": "H9cZEo9Dckwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### detection using yolov8"
      ],
      "metadata": {
        "id": "B6P2fNAudVNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install supervision"
      ],
      "metadata": {
        "id": "GBfN4Dwkdo9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load displayer\n",
        "from IPython.display import display\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "url = \"https://images.rawpixel.com/image_800/czNmcy1wcml2YXRlL3Jhd3BpeGVsX2ltYWdlcy93ZWJzaXRlX2NvbnRlbnQvZnJob3JzZV9nYWxsb3BfY2FudGVyX21hcmUtaW1hZ2Utcm01MDNfMS1sMDd0dW5iZy5qcGc.jpg?s=DyXVyF2nJuzRikNn0KXYhzn7TwTJhuaRG1WoOQqftgQ\"\n",
        "\n",
        "# Define the URL of the image\n",
        "image_url = url\n",
        "\n",
        "# Send an HTTP GET request to the URL\n",
        "response = requests.get(image_url)\n",
        "\n",
        "# Check if the request was successful (HTTP status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Read the image data from the response content\n",
        "    image_data = np.frombuffer(response.content, np.uint8)\n",
        "\n",
        "    # Decode the image using OpenCV\n",
        "    img = cv2.imdecode(image_data, cv2.IMREAD_COLOR)\n",
        "\n",
        "else:\n",
        "    print(\"Failed to fetch the image. HTTP Status Code:\", response.status_code)\n",
        "image=img\n",
        "#cv2.imwrite(\"test.png\",image)"
      ],
      "metadata": {
        "id": "4pia_kSfdrJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import supervision as sv\n",
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolov8l.pt')\n",
        "\n",
        "results = model(image)\n",
        "\n",
        "print(results[0].boxes)\n",
        "detections = sv.Detections.from_ultralytics(results[0])\n",
        "\n",
        "#image = #cv2.imread(\"/home/swagroy/car_images/CAR1.PNG\")\n",
        "\n",
        "bounding_box_annotator = sv.BoundingBoxAnnotator()\n",
        "label_annotator = sv.LabelAnnotator()\n",
        "\n",
        "classes = model.names\n",
        "\n",
        "labels = [\n",
        "\tf\"{classes[class_id]} {confidence:0.2f}\"\n",
        "\tfor _, _, confidence, class_id, _\n",
        "\tin detections\n",
        "]\n",
        "\n",
        "annotated_image = bounding_box_annotator.annotate(\n",
        "    scene=image, detections=detections)\n",
        "annotated_image = label_annotator.annotate(\n",
        "    scene=annotated_image, detections=detections, labels=labels)\n",
        "\n",
        "sv.plot_image(annotated_image)\n",
        "\n",
        "print(labels)"
      ],
      "metadata": {
        "id": "_-sj3bhQdYox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load displayer\n",
        "from IPython.display import display\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "url = \"http://images.cocodataset.org/test-stuff2017/000000009384.jpg\"\n",
        "\n",
        "# Define the URL of the image\n",
        "image_url = url\n",
        "\n",
        "# Send an HTTP GET request to the URL\n",
        "response = requests.get(image_url)\n",
        "\n",
        "# Check if the request was successful (HTTP status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Read the image data from the response content\n",
        "    image_data = np.frombuffer(response.content, np.uint8)\n",
        "\n",
        "    # Decode the image using OpenCV\n",
        "    img = cv2.imdecode(image_data, cv2.IMREAD_COLOR)\n",
        "\n",
        "else:\n",
        "    print(\"Failed to fetch the image. HTTP Status Code:\", response.status_code)\n",
        "image=img\n",
        "import supervision as sv\n",
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolov8l.pt')\n",
        "\n",
        "results = model(image)\n",
        "\n",
        "print(results[0].boxes)\n",
        "detections = sv.Detections.from_ultralytics(results[0])\n",
        "\n",
        "#image = #cv2.imread(\"/home/swagroy/car_images/CAR1.PNG\")\n",
        "\n",
        "bounding_box_annotator = sv.BoundingBoxAnnotator()\n",
        "label_annotator = sv.LabelAnnotator()\n",
        "\n",
        "classes = model.names\n",
        "\n",
        "labels = [\n",
        "\tf\"{classes[class_id]} {confidence:0.2f}\"\n",
        "\tfor _, _, confidence, class_id, _\n",
        "\tin detections\n",
        "]\n",
        "\n",
        "annotated_image = bounding_box_annotator.annotate(\n",
        "    scene=image, detections=detections)\n",
        "annotated_image = label_annotator.annotate(\n",
        "    scene=annotated_image, detections=detections, labels=labels)\n",
        "\n",
        "sv.plot_image(annotated_image)\n",
        "\n",
        "print(labels)"
      ],
      "metadata": {
        "id": "Iu3I7dfreC2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv2.imwrite(\"/content/test.png\",image)\n",
        "img_path='/content/test.png'\n",
        "\n",
        "print(f\"nlpconnect/vit-gpt2-image-captioning caption: {get_caption(finetuned_model, finetuned_image_processor, finetuned_tokenizer, img_path)}\")\n",
        "print(f\"Abdou/vit-swin-base-224-gpt2-image-captioning caption: {get_caption(image_captioner.model,  finetuned_image_processor, finetuned_tokenizer,  img_path)}\")"
      ],
      "metadata": {
        "id": "0rwGwGXsebFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## yolov5"
      ],
      "metadata": {
        "id": "Siw6IyvvcFJS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZR5MakMZyn8g"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ultralytics/yolov5.git\n",
        "%cd yolov5\n",
        "!wget https://github.com/ultralytics/yolov5/releases/download/v5.0/yolov5s.pt\n",
        "!wget https://github.com/ultralytics/yolov5/releases/download/v5.0/yolov5m.pt\n",
        "!wget https://github.com/ultralytics/yolov5/releases/download/v5.0/yolov5l.pt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "# Load the model\n",
        "model1 = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
        "model2= torch.hub.load('ultralytics/yolov5', 'yolov5m', pretrained=True)\n",
        "model3= torch.hub.load('ultralytics/yolov5', 'yolov5l', pretrained=True)"
      ],
      "metadata": {
        "id": "qbkwHQrkzbRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# implementations using yolov5"
      ],
      "metadata": {
        "id": "SKlTHaLB3CmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load displayer\n",
        "from IPython.display import display\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "url = \"https://images.rawpixel.com/image_800/czNmcy1wcml2YXRlL3Jhd3BpeGVsX2ltYWdlcy93ZWJzaXRlX2NvbnRlbnQvZnJob3JzZV9nYWxsb3BfY2FudGVyX21hcmUtaW1hZ2Utcm01MDNfMS1sMDd0dW5iZy5qcGc.jpg?s=DyXVyF2nJuzRikNn0KXYhzn7TwTJhuaRG1WoOQqftgQ\"\n",
        "\n",
        "# Define the URL of the image\n",
        "image_url = url\n",
        "\n",
        "# Send an HTTP GET request to the URL\n",
        "response = requests.get(image_url)\n",
        "\n",
        "# Check if the request was successful (HTTP status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Read the image data from the response content\n",
        "    image_data = np.frombuffer(response.content, np.uint8)\n",
        "\n",
        "    # Decode the image using OpenCV\n",
        "    img = cv2.imdecode(image_data, cv2.IMREAD_COLOR)\n",
        "\n",
        "else:\n",
        "    print(\"Failed to fetch the image. HTTP Status Code:\", response.status_code)\n",
        "image=img\n",
        "cv2.imwrite(\"test.png\",image)\n",
        "img_path='/content/yolov5/test.png'\n",
        "# Perform inference\n",
        "results1 = model1(img_path)\n",
        "\n",
        "# Display results\n",
        "results1.show()\n",
        "\n",
        "# Perform inference\n",
        "results2 = model2(img_path)\n",
        "\n",
        "# Display results\n",
        "results2.show()\n",
        "\n",
        "\n",
        "print(f\"nlpconnect/vit-gpt2-image-captioning caption: {get_caption(finetuned_model, finetuned_image_processor, finetuned_tokenizer, img_path)}\")\n",
        "print(f\"Abdou/vit-swin-base-224-gpt2-image-captioning caption: {get_caption(image_captioner.model,  finetuned_image_processor, finetuned_tokenizer,  img_path)}\")"
      ],
      "metadata": {
        "id": "VPetUzmkzqh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load displayer\n",
        "from IPython.display import display\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "url = \"http://images.cocodataset.org/test-stuff2017/000000009384.jpg\"\n",
        "\n",
        "# Define the URL of the image\n",
        "image_url = url\n",
        "\n",
        "# Send an HTTP GET request to the URL\n",
        "response = requests.get(image_url)\n",
        "\n",
        "# Check if the request was successful (HTTP status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Read the image data from the response content\n",
        "    image_data = np.frombuffer(response.content, np.uint8)\n",
        "\n",
        "    # Decode the image using OpenCV\n",
        "    img = cv2.imdecode(image_data, cv2.IMREAD_COLOR)\n",
        "\n",
        "else:\n",
        "    print(\"Failed to fetch the image. HTTP Status Code:\", response.status_code)\n",
        "image=img\n",
        "cv2.imwrite(\"test.png\",image)\n",
        "img_path='/content/yolov5/test.png'\n",
        "# Perform inference\n",
        "results1 = model1(img_path)\n",
        "\n",
        "# Display results\n",
        "results1.show()\n",
        "\n",
        "# Perform inference\n",
        "results2 = model2(img_path)\n",
        "\n",
        "# Display results\n",
        "results2.show()\n",
        "\n",
        "get_caption(finetuned_model, finetuned_image_processor, finetuned_tokenizer, img_path)\n",
        "print(f\"nlpconnect/vit-gpt2-image-captioning caption: {get_caption(finetuned_model, finetuned_image_processor, finetuned_tokenizer, img_path)}\")\n",
        "print(f\"Abdou/vit-swin-base-224-gpt2-image-captioning caption: {get_caption(image_captioner.model,  finetuned_image_processor, finetuned_tokenizer,  img_path)}\")"
      ],
      "metadata": {
        "id": "CCWjqSRL3FMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load displayer\n",
        "from IPython.display import display\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "url = \"https://akm-img-a-in.tosshub.com/businesstoday/images/story/202303/r_0-sixteen_nine.jpg?size=1200:675\"\n",
        "\n",
        "# Define the URL of the image\n",
        "image_url = url\n",
        "\n",
        "# Send an HTTP GET request to the URL\n",
        "response = requests.get(image_url)\n",
        "\n",
        "# Check if the request was successful (HTTP status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Read the image data from the response content\n",
        "    image_data = np.frombuffer(response.content, np.uint8)\n",
        "\n",
        "    # Decode the image using OpenCV\n",
        "    img = cv2.imdecode(image_data, cv2.IMREAD_COLOR)\n",
        "\n",
        "else:\n",
        "    print(\"Failed to fetch the image. HTTP Status Code:\", response.status_code)\n",
        "image=img\n",
        "cv2.imwrite(\"test.png\",image)\n",
        "img_path='/content/yolov5/test.png'\n",
        "# Perform inference\n",
        "results1 = model1(img_path)\n",
        "\n",
        "# Display results\n",
        "results1.show()\n",
        "\n",
        "# Perform inference\n",
        "results2 = model2(img_path)\n",
        "\n",
        "# Display results\n",
        "results2.show()\n",
        "\n",
        "get_caption(finetuned_model, finetuned_image_processor, finetuned_tokenizer, img_path)\n",
        "print(f\"nlpconnect/vit-gpt2-image-captioning caption: {get_caption(finetuned_model, finetuned_image_processor, finetuned_tokenizer, img_path)}\")\n",
        "print(f\"Abdou/vit-swin-base-224-gpt2-image-captioning caption: {get_caption(image_captioner.model,  finetuned_image_processor, finetuned_tokenizer,  img_path)}\")"
      ],
      "metadata": {
        "id": "f_s8dcZW1QQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load displayer\n",
        "from IPython.display import display\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "url = \"https://t3.ftcdn.net/jpg/03/36/97/58/360_F_336975809_VvYkV1QZX2E8igeS3kYpcBGiMcK6zWpL.jpg\"\n",
        "\n",
        "# Define the URL of the image\n",
        "image_url = url\n",
        "\n",
        "# Send an HTTP GET request to the URL\n",
        "response = requests.get(image_url)\n",
        "\n",
        "# Check if the request was successful (HTTP status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Read the image data from the response content\n",
        "    image_data = np.frombuffer(response.content, np.uint8)\n",
        "\n",
        "    # Decode the image using OpenCV\n",
        "    img = cv2.imdecode(image_data, cv2.IMREAD_COLOR)\n",
        "\n",
        "else:\n",
        "    print(\"Failed to fetch the image. HTTP Status Code:\", response.status_code)\n",
        "image=img\n",
        "cv2.imwrite(\"test.png\",image)\n",
        "img_path='/content/yolov5/test.png'\n",
        "# Perform inference\n",
        "results1 = model1(img_path)\n",
        "\n",
        "# Display results\n",
        "results1.show()\n",
        "\n",
        "# Perform inference\n",
        "results2 = model2(img_path)\n",
        "\n",
        "# Display results\n",
        "results2.show()\n",
        "# Perform inference\n",
        "results3 = model3(img_path)\n",
        "\n",
        "# Display results\n",
        "results3.show()\n",
        "\n",
        "get_caption(finetuned_model, finetuned_image_processor, finetuned_tokenizer, img_path)\n",
        "print(f\"nlpconnect/vit-gpt2-image-captioning caption: {get_caption(finetuned_model, finetuned_image_processor, finetuned_tokenizer, img_path)}\")\n",
        "print(f\"Abdou/vit-swin-base-224-gpt2-image-captioning caption: {get_caption(image_captioner.model,  finetuned_image_processor, finetuned_tokenizer,  img_path)}\")"
      ],
      "metadata": {
        "id": "nqQR6t-N3YoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load displayer\n",
        "from IPython.display import display\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "url = \"https://images.hindustantimes.com/auto/img/2021/12/28/600x338/Indian_cars_1640662074513_1640662081298.jpg\"\n",
        "\n",
        "# Define the URL of the image\n",
        "image_url = url\n",
        "\n",
        "# Send an HTTP GET request to the URL\n",
        "response = requests.get(image_url)\n",
        "\n",
        "# Check if the request was successful (HTTP status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Read the image data from the response content\n",
        "    image_data = np.frombuffer(response.content, np.uint8)\n",
        "\n",
        "    # Decode the image using OpenCV\n",
        "    img = cv2.imdecode(image_data, cv2.IMREAD_COLOR)\n",
        "\n",
        "else:\n",
        "    print(\"Failed to fetch the image. HTTP Status Code:\", response.status_code)\n",
        "image=img\n",
        "cv2.imwrite(\"test.png\",image)\n",
        "img_path='/content/yolov5/test.png'\n",
        "# Perform inference\n",
        "results1 = model1(img_path)\n",
        "\n",
        "# Display results\n",
        "results1.show()\n",
        "\n",
        "# Perform inference\n",
        "results2 = model2(img_path)\n",
        "\n",
        "# Display results\n",
        "results2.show()\n",
        "\n",
        "get_caption(finetuned_model, finetuned_image_processor, finetuned_tokenizer, img_path)\n",
        "print(f\"nlpconnect/vit-gpt2-image-captioning caption: {get_caption(finetuned_model, finetuned_image_processor, finetuned_tokenizer, img_path)}\")\n",
        "print(f\"Abdou/vit-swin-base-224-gpt2-image-captioning caption: {get_caption(image_captioner.model,  finetuned_image_processor, finetuned_tokenizer,  img_path)}\")"
      ],
      "metadata": {
        "id": "ZwpsnNu03Y4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load displayer\n",
        "from IPython.display import display\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "url = \"https://cms.londonzoo.org/sites/default/files/styles/responsive/public/1024/729/1/2022-11/Asim-at-London-Zoo.jpg\"\n",
        "\n",
        "# Define the URL of the image\n",
        "image_url = url\n",
        "\n",
        "# Send an HTTP GET request to the URL\n",
        "response = requests.get(image_url)\n",
        "\n",
        "# Check if the request was successful (HTTP status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Read the image data from the response content\n",
        "    image_data = np.frombuffer(response.content, np.uint8)\n",
        "\n",
        "    # Decode the image using OpenCV\n",
        "    img = cv2.imdecode(image_data, cv2.IMREAD_COLOR)\n",
        "\n",
        "else:\n",
        "    print(\"Failed to fetch the image. HTTP Status Code:\", response.status_code)\n",
        "image=img\n",
        "cv2.imwrite(\"test.png\",image)\n",
        "img_path='/content/yolov5/test.png'\n",
        "# Perform inference\n",
        "results1 = model1(img_path)\n",
        "\n",
        "# Display results\n",
        "results1.show()\n",
        "\n",
        "# Perform inference\n",
        "results2 = model2(img_path)\n",
        "\n",
        "# Display results\n",
        "results2.show()\n",
        "\n",
        "# Perform inference\n",
        "results3 = model3(img_path)\n",
        "\n",
        "# Display results\n",
        "results3.show()\n",
        "\n",
        "get_caption(finetuned_model, finetuned_image_processor, finetuned_tokenizer, img_path)\n",
        "print(f\"nlpconnect/vit-gpt2-image-captioning caption: {get_caption(finetuned_model, finetuned_image_processor, finetuned_tokenizer, img_path)}\")\n",
        "print(f\"Abdou/vit-swin-base-224-gpt2-image-captioning caption: {get_caption(image_captioner.model,  finetuned_image_processor, finetuned_tokenizer,  img_path)}\")"
      ],
      "metadata": {
        "id": "3bBbSmvx3ZGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load displayer\n",
        "from IPython.display import display\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "url = \"https://i.ytimg.com/vi/BQNRE2ScAq4/maxresdefault.jpg\"\n",
        "\n",
        "# Define the URL of the image\n",
        "image_url = url\n",
        "\n",
        "# Send an HTTP GET request to the URL\n",
        "response = requests.get(image_url)\n",
        "\n",
        "# Check if the request was successful (HTTP status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Read the image data from the response content\n",
        "    image_data = np.frombuffer(response.content, np.uint8)\n",
        "\n",
        "    # Decode the image using OpenCV\n",
        "    img = cv2.imdecode(image_data, cv2.IMREAD_COLOR)\n",
        "\n",
        "else:\n",
        "    print(\"Failed to fetch the image. HTTP Status Code:\", response.status_code)\n",
        "image=img\n",
        "cv2.imwrite(\"test.png\",image)\n",
        "img_path='/content/yolov5/test.png'\n",
        "# Perform inference\n",
        "results1 = model1(img_path)\n",
        "\n",
        "# Display results\n",
        "results1.show()\n",
        "\n",
        "# Perform inference\n",
        "results2 = model2(img_path)\n",
        "\n",
        "# Display results\n",
        "results2.show()\n",
        "\n",
        "# Perform inference\n",
        "results3 = model3(img_path)\n",
        "\n",
        "# Display results\n",
        "results3.show()\n",
        "\n",
        "\n",
        "get_caption(finetuned_model, finetuned_image_processor, finetuned_tokenizer, img_path)\n",
        "print(f\"nlpconnect/vit-gpt2-image-captioning caption: {get_caption(finetuned_model, finetuned_image_processor, finetuned_tokenizer, img_path)}\")\n",
        "print(f\"Abdou/vit-swin-base-224-gpt2-image-captioning caption: {get_caption(image_captioner.model,  finetuned_image_processor, finetuned_tokenizer,  img_path)}\")"
      ],
      "metadata": {
        "id": "zpAiqARP4SxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load displayer\n",
        "from IPython.display import display\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "url = \"https://ihearthorses.com/wp-content/uploads/2020/05/Canva-dog-in-a-cowboy-hat-holding-horse-on-leash-1-scaled.jpg\"\n",
        "\n",
        "# Define the URL of the image\n",
        "image_url = url\n",
        "\n",
        "# Send an HTTP GET request to the URL\n",
        "response = requests.get(image_url)\n",
        "\n",
        "# Check if the request was successful (HTTP status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Read the image data from the response content\n",
        "    image_data = np.frombuffer(response.content, np.uint8)\n",
        "\n",
        "    # Decode the image using OpenCV\n",
        "    img = cv2.imdecode(image_data, cv2.IMREAD_COLOR)\n",
        "\n",
        "else:\n",
        "    print(\"Failed to fetch the image. HTTP Status Code:\", response.status_code)\n",
        "image=img\n",
        "cv2.imwrite(\"test.png\",image)\n",
        "img_path='/content/yolov5/test.png'\n",
        "# Perform inference\n",
        "results1 = model1(img_path)\n",
        "\n",
        "# Display results\n",
        "results1.show()\n",
        "\n",
        "# Perform inference\n",
        "results2 = model2(img_path)\n",
        "\n",
        "# Display results\n",
        "results2.show()\n",
        "\n",
        "get_caption(finetuned_model, finetuned_image_processor, finetuned_tokenizer, img_path)\n",
        "print(f\"nlpconnect/vit-gpt2-image-captioning caption: {get_caption(finetuned_model, finetuned_image_processor, finetuned_tokenizer, img_path)}\")\n",
        "print(f\"Abdou/vit-swin-base-224-gpt2-image-captioning caption: {get_caption(image_captioner.model,  finetuned_image_processor, finetuned_tokenizer,  img_path)}\")"
      ],
      "metadata": {
        "id": "J--R1ULc4qjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load displayer\n",
        "from IPython.display import display\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "url = \"https://img.freepik.com/premium-photo/amazing-italian-landscapes-lombardy-scenaries_526992-280.jpg?w=2000\"\n",
        "\n",
        "# Define the URL of the image\n",
        "image_url = url\n",
        "\n",
        "# Send an HTTP GET request to the URL\n",
        "response = requests.get(image_url)\n",
        "\n",
        "# Check if the request was successful (HTTP status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Read the image data from the response content\n",
        "    image_data = np.frombuffer(response.content, np.uint8)\n",
        "\n",
        "    # Decode the image using OpenCV\n",
        "    img = cv2.imdecode(image_data, cv2.IMREAD_COLOR)\n",
        "\n",
        "else:\n",
        "    print(\"Failed to fetch the image. HTTP Status Code:\", response.status_code)\n",
        "image=img\n",
        "cv2.imwrite(\"test.png\",image)\n",
        "img_path='/content/yolov5/test.png'\n",
        "# Perform inference\n",
        "results1 = model1(img_path)\n",
        "\n",
        "# Display results\n",
        "results1.show()\n",
        "\n",
        "# Perform inference\n",
        "results2 = model2(img_path)\n",
        "\n",
        "# Display results\n",
        "results2.show()\n",
        "\n",
        "# Perform inference\n",
        "results3= model3(img_path)\n",
        "\n",
        "# Display results\n",
        "results3.show()\n",
        "\n",
        "get_caption(finetuned_model, finetuned_image_processor, finetuned_tokenizer, img_path)\n",
        "print(f\"nlpconnect/vit-gpt2-image-captioning caption: {get_caption(finetuned_model, finetuned_image_processor, finetuned_tokenizer, img_path)}\")\n",
        "print(f\"Abdou/vit-swin-base-224-gpt2-image-captioning caption: {get_caption(image_captioner.model,  finetuned_image_processor, finetuned_tokenizer,  img_path)}\")"
      ],
      "metadata": {
        "id": "znw7zxJL4BFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load displayer\n",
        "from IPython.display import display\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "url = \"https://i.pinimg.com/474x/c5/6c/cb/c56ccb04377c2905fbb8f9ec00b7ae2f.jpg\"\n",
        "\n",
        "# Define the URL of the image\n",
        "image_url = url\n",
        "\n",
        "# Send an HTTP GET request to the URL\n",
        "response = requests.get(image_url)\n",
        "\n",
        "# Check if the request was successful (HTTP status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Read the image data from the response content\n",
        "    image_data = np.frombuffer(response.content, np.uint8)\n",
        "\n",
        "    # Decode the image using OpenCV\n",
        "    img = cv2.imdecode(image_data, cv2.IMREAD_COLOR)\n",
        "\n",
        "else:\n",
        "    print(\"Failed to fetch the image. HTTP Status Code:\", response.status_code)\n",
        "image=img\n",
        "cv2.imwrite(\"test.png\",image)\n",
        "img_path='/content/yolov5/test.png'\n",
        "# Perform inference\n",
        "results1 = model1(img_path)\n",
        "\n",
        "# Display results\n",
        "results1.show()\n",
        "\n",
        "# Perform inference\n",
        "results2 = model2(img_path)\n",
        "\n",
        "# Display results\n",
        "results2.show()\n",
        "\n",
        "# Perform inference\n",
        "results3= model3(img_path)\n",
        "\n",
        "# Display results\n",
        "results3.show()\n",
        "\n",
        "get_caption(finetuned_model, finetuned_image_processor, finetuned_tokenizer, img_path)\n",
        "print(f\"nlpconnect/vit-gpt2-image-captioning caption: {get_caption(finetuned_model, finetuned_image_processor, finetuned_tokenizer, img_path)}\")\n",
        "print(f\"Abdou/vit-swin-base-224-gpt2-image-captioning caption: {get_caption(image_captioner.model,  finetuned_image_processor, finetuned_tokenizer,  img_path)}\")"
      ],
      "metadata": {
        "id": "kFIEYwx_8xyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load displayer\n",
        "from IPython.display import display\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "url = \"https://nmaahc.si.edu/sites/default/files/styles/max_1300x1300/public/images/header/audience-citizen_0.jpg?itok=unjNTfkP\"\n",
        "\n",
        "# Define the URL of the image\n",
        "image_url = url\n",
        "\n",
        "# Send an HTTP GET request to the URL\n",
        "response = requests.get(image_url)\n",
        "\n",
        "# Check if the request was successful (HTTP status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Read the image data from the response content\n",
        "    image_data = np.frombuffer(response.content, np.uint8)\n",
        "\n",
        "    # Decode the image using OpenCV\n",
        "    img = cv2.imdecode(image_data, cv2.IMREAD_COLOR)\n",
        "\n",
        "else:\n",
        "    print(\"Failed to fetch the image. HTTP Status Code:\", response.status_code)\n",
        "image=img\n",
        "cv2.imwrite(\"test.png\",image)\n",
        "img_path='/content/yolov5/test.png'\n",
        "# Perform inference\n",
        "results1 = model1(img_path)\n",
        "\n",
        "# Display results\n",
        "results1.show()\n",
        "\n",
        "# Perform inference\n",
        "results2 = model2(img_path)\n",
        "\n",
        "# Display results\n",
        "results2.show()\n",
        "\n",
        "# Perform inference\n",
        "results3= model3(img_path)\n",
        "\n",
        "# Display results\n",
        "results3.show()\n",
        "\n",
        "get_caption(finetuned_model, finetuned_image_processor, finetuned_tokenizer, img_path)\n",
        "print(f\"nlpconnect/vit-gpt2-image-captioning caption: {get_caption(finetuned_model, finetuned_image_processor, finetuned_tokenizer, img_path)}\")\n",
        "print(f\"Abdou/vit-swin-base-224-gpt2-image-captioning caption: {get_caption(image_captioner.model,  finetuned_image_processor, finetuned_tokenizer,  img_path)}\")"
      ],
      "metadata": {
        "id": "yXELPrCK9GLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# vit custom model"
      ],
      "metadata": {
        "id": "_7n7Triw30VA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MkxbfCXM35r5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXnhGBaRvniK"
      },
      "source": [
        "# Image Captioning using ViT and GPT2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrY1z0fMwksc"
      },
      "source": [
        "Connect to Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGUR5I4rwg0E"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcrtZhJdNy1Y"
      },
      "outputs": [],
      "source": [
        "!rm -r '/content/image_caption_gen' '/content/wandb'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdD3HuuqCNCN"
      },
      "outputs": [],
      "source": [
        "!pip install -q accelerate wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T64UIQddsu9Y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from transformers import AutoFeatureExtractor, AutoTokenizer, VisionEncoderDecoderModel\n",
        "from PIL import Image\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, default_data_collator, EarlyStoppingCallback\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msZpGxgjvnHf"
      },
      "outputs": [],
      "source": [
        "img_dir = '/content/Images'\n",
        "caption_path = '/content/captions.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eojVbtwwxpHt"
      },
      "outputs": [],
      "source": [
        "caption_data = pd.read_csv(caption_path)\n",
        "caption_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVTpff4oxy4j"
      },
      "outputs": [],
      "source": [
        "def text_preprocessing(data):\n",
        "    data['caption'] = data['caption'].apply(lambda x: x.lower())\n",
        "    data['caption'] = data['caption'].apply(lambda x: x.replace(\"[^A-Za-z]\",\"\"))\n",
        "    data['caption'] = data['caption'].apply(lambda x: x.replace(\"\\s+\",\" \"))\n",
        "    data['caption'] = data['caption'].apply(lambda x: \" \".join([word for word in x.split() if len(word)>1]))\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYGn5gc-yvDj"
      },
      "outputs": [],
      "source": [
        "caption_data_preprocessed = text_preprocessing(caption_data)\n",
        "caption_data_preprocessed.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ko0fM7zTy5Q2"
      },
      "outputs": [],
      "source": [
        "enc_model = \"google/vit-base-patch16-224-in21k\"\n",
        "dec_model = \"gpt2\"\n",
        "\n",
        "fe_extractor = AutoFeatureExtractor.from_pretrained(enc_model)\n",
        "tokenizer = AutoTokenizer.from_pretrained(dec_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkqhiOZfzUSE"
      },
      "outputs": [],
      "source": [
        "#set the padding token to eos token for fixed length input tokens\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNNPbOw60EOd"
      },
      "source": [
        "Sample one image transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_kr1di6zhKo"
      },
      "outputs": [],
      "source": [
        "max_length = 128\n",
        "sample = caption_data_preprocessed.iloc[0]\n",
        "\n",
        "#fetch the image\n",
        "image = Image.open(os.path.join(img_dir,sample['image'])).convert('RGB')\n",
        "caption = sample['caption']\n",
        "\n",
        "#apply the feature extractor on the image\n",
        "inputs = fe_extractor(image, return_tensors='pt')\n",
        "#apply the tokenizer on the caption\n",
        "outputs = tokenizer(caption, max_length=max_length, truncation=True,\n",
        "                    padding='max_length', return_tensors = 'pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7ZWHqOA77Do"
      },
      "outputs": [],
      "source": [
        "inputs['pixel_values'].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np3kV1LJ1Za-"
      },
      "source": [
        "## Create the Dataset class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q35ruE0Q0dP5"
      },
      "outputs": [],
      "source": [
        "class ImageCaptionDataset(Dataset):\n",
        "  def __init__(self, data):\n",
        "    self.images = data['image'].values\n",
        "    self.captions = data['caption'].values\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    input_data = {}\n",
        "    #load the image and tokenize it\n",
        "    image = Image.open(os.path.join(img_dir,str(self.images[idx]))).convert('RGB')\n",
        "    image_embed = fe_extractor(image, return_tensors = 'pt')\n",
        "\n",
        "    #load caption and apply tokenizer\n",
        "    caption = self.captions[idx]\n",
        "    captions_tok = tokenizer(caption, max_length=max_length,\n",
        "                             truncation=True, padding='max_length',\n",
        "                             return_tensors = 'pt')['input_ids'][0]\n",
        "\n",
        "    #store the image_embeddings and caption_tok in the dict\n",
        "    input_data['pixel_values'] = image_embed['pixel_values'].squeeze()\n",
        "    input_data['labels'] = captions_tok\n",
        "    return input_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQvN2l03_ji0"
      },
      "source": [
        "Split the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEPBC8u708Bh"
      },
      "outputs": [],
      "source": [
        "\n",
        "X, y = train_test_split(caption_data_preprocessed, test_size=0.2,\n",
        "                        shuffle=True, random_state=42)\n",
        "train_dataset = ImageCaptionDataset(X)\n",
        "test_dataset = ImageCaptionDataset(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8T_utEFAC8l"
      },
      "source": [
        "## Model Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kEy2vNdABXM"
      },
      "outputs": [],
      "source": [
        "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
        "    enc_model,\n",
        "    dec_model\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPjHZF8nAWAt"
      },
      "source": [
        "Set the decoder_start_token_id and the pad_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8cw8MKCAT1l"
      },
      "outputs": [],
      "source": [
        "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkDJroaKBM_x"
      },
      "source": [
        "## Setting up the training configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kcY4MFCBKNA"
      },
      "outputs": [],
      "source": [
        "# early_stopping_callback = EarlyStoppingCallback(\n",
        "#     early_stopping_patience= 1,\n",
        "#     early_stopping_threshold= 1e-3\n",
        "# )\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    run_name = 'img_cap_ViT_gpt2_run_5',\n",
        "    output_dir = 'image_caption_gen',\n",
        "    evaluation_strategy = 'epoch',\n",
        "    logging_strategy='steps',\n",
        "    logging_steps=5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    learning_rate=3e-2,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_steps=10,\n",
        "    weight_decay=0.05,\n",
        "    report_to='wandb',\n",
        "    num_train_epochs = 3,\n",
        "    save_strategy='epoch',\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    tokenizer = fe_extractor,\n",
        "    data_collator=default_data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset = test_dataset,\n",
        "    args = training_args,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "O0OywDKmCEU1"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MhZgsMnRAWG8"
      },
      "outputs": [],
      "source": [
        "cp -r /content/image_caption_gen /content/drive/MyDrive/visual_multimodal_modelling"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ipuWiz5V8ip6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1qLCQrKC2XK"
      },
      "source": [
        "## Inferencing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = VisionEncoderDecoderModel.from_pretrained(\n",
        "    '/content/drive/MyDrive/visual_multimodal_modelling/image_caption_gen/checkpoint-6069'\n",
        ")"
      ],
      "metadata": {
        "id": "CILhrfpcdLpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nfSj59CBA-x"
      },
      "source": [
        "## Set the beam search params for inferencing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSaAQi5DA9UT"
      },
      "outputs": [],
      "source": [
        "num_beams = 4\n",
        "model.config.num_beams = num_beams"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = test_dataset[65]['pixel_values']"
      ],
      "metadata": {
        "id": "s0urmTTmeV2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIBSw_3qC3xl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # uncomment the below line if feature extractor is not applied to the image already\n",
        "    # inputs = fe_extractor(images=inputs, return_tensors='pt').pixel_values\n",
        "\n",
        "    # generate caption for the image\n",
        "    out = model.generate(\n",
        "        inputs.unsqueeze(0).to('cuda') if torch.cuda.is_available() else inputs.unsqueeze(0).to('cpu'), # move inputs to GPU\n",
        "        num_beams=num_beams,\n",
        "        )\n",
        "\n",
        "# convert token ids to string format\n",
        "decoded_out = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "print(decoded_out)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bHO0AazteenD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}